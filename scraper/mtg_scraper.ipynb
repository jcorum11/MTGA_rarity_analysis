{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all packages necessary for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import timeit\n",
    "import filecmp\n",
    "import url_stripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a webcrawler to gather the data from mtgtop8.com on competitive decks\n",
    "url = 'https://www.mtgtop8.com/format?f=ST'\n",
    "url_standard_page_response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the response\n",
    "with open('C:\\\\Users\\\\muroc\\\\Documents\\\\MTG\\\\data\\\\url_standard_page_response.html', mode='wb') as file:\n",
    "    file.write(url_standard_page_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open response as a BeautifulSoup object\n",
    "with open('C:\\\\Users\\\\muroc\\\\Documents\\\\MTG\\\\data\\\\url_standard_page_response.html') as file:\n",
    "    soup = BeautifulSoup(file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all urls for deck types and put them into a dataframe\n",
    "url = {'url': soup.find_all(href=re.compile(r\"archetype\\?a\"))}\n",
    "deck_type_urls = pd.DataFrame(data=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for deck type and clean the column\n",
    "url_stripper.url_stripper(deck_type_urls, 'url', 'type', '(>.+<)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract url from the url column\n",
    "deck_type_urls['url'] = deck_type_urls.url.str.extract('(archetype.+f\\=ST)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add root url to the data\n",
    "urls = []\n",
    "\n",
    "for i in np.arange(deck_type_urls.shape[0]):\n",
    "    urls.append(str('https://www.mtgtop8.com/') + deck_type_urls.url[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_type_urls['url'] = urls\n",
    "deck_type_urls['type'] = deck_type_urls.type.str.replace(' ', '_')\n",
    "deck_type_urls['type'] = deck_type_urls.type.str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create web crawler to request html pages from deck_type_urls\n",
    "# create new path and name it newpath\n",
    "newpath = 'C:\\\\Users\\\\muroc\\\\Documents\\\\MTG\\\\type_html_files'\n",
    "\n",
    "# check to make sure that there is no path that matches newpath and create a folder called html_files if there isn't\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "# change current directory to newpath\n",
    "os.chdir(newpath)\n",
    "\n",
    "# scrape all html files from urls in deck_type_urls and put them in html_files folder\n",
    "for i in np.arange(deck_type_urls.shape[0]):\n",
    "    type_html = requests.get(deck_type_urls.url[i])\n",
    "    with open(deck_type_urls.type[i], 'wb') as file:\n",
    "        file.write(type_html.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of file names in type_html_files folder and name it dir_names\n",
    "dir_names = []\n",
    "cur_dir = 'C:/Users/muroc/Documents/MTG/type_html_files/'\n",
    "\n",
    "with os.scandir(cur_dir) as folder:\n",
    "    for file in folder: \n",
    "        if file.is_file():\n",
    "            dir_names.append(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all urls in all files\n",
    "players = []\n",
    "urls = []\n",
    "\n",
    "for i in np.arange(np.count_nonzero(dir_names)):\n",
    "    with open('C:/Users/muroc/Documents/MTG/type_html_files/' + dir_names[i]) as file:\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "    for a in soup.find_all(href=re.compile(r'search\\?player')):\n",
    "        players.append(a)\n",
    "    for a in soup.find_all(href=re.compile(r'event\\?e\\=.+\\&d\\=.+\\&f\\=ST')):\n",
    "        urls.append(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_urls = pd.DataFrame({'player': players, \n",
    "                         'url': urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get deck names and clean them up\n",
    "url_stripper.url_stripper(deck_urls, 'url', 'name', '(>.+<)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract url from the url column\n",
    "deck_urls['url'] = deck_urls.url.str.extract('(event.+f\\=ST)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add root url to the data\n",
    "deck_urls.url = 'https://www.mtgtop8.com/' + deck_urls.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean urls and name\n",
    "deck_urls['name'] = deck_urls.name.str.replace(' ', '_')\n",
    "deck_urls['name'] = deck_urls.name.str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stripper.url_stripper(deck_urls, 'player', 'player', '(>.+<)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_urls.player = deck_urls.player.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a web crawler to request html pages from deck_type_urls\n",
    "\n",
    "# create a new path and name it newpath\n",
    "newpath = 'C:/Users/muroc/Documents/MTG/html_files'\n",
    "\n",
    "# check to make sure that there is no path that matches newpath and create a folder called html_files if there isn't\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "# change the current directory to newpath\n",
    "os.chdir(newpath)\n",
    "\n",
    "# scrape all html files from urls in deck_urls and put them in html_files folder\n",
    "deck_errors = []\n",
    "\n",
    "for i in np.arange(deck_urls.shape[0]):\n",
    "    try:\n",
    "        deck_html = requests.get(deck_urls.url[i])\n",
    "        with open(str(deck_urls.player[i]) + '_' + str(deck_urls.name[i]) + '.html', 'wb') as file:\n",
    "            file.write(deck_html.content)\n",
    "    except Exception as e:\n",
    "        print(str(i) + ' ' + str(e))\n",
    "        deck_errors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all directory directory names and put them in a list\n",
    "dir_names = []\n",
    "curdir = 'C:/Users/muroc/Documents/MTG/html_files'\n",
    "\n",
    "with os.scandir(curdir) as folder:\n",
    "    for file in folder:\n",
    "        if file.is_file():\n",
    "            dir_names.append(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take names and add them to curdir and open them and find cards and players with beautifulsoup \n",
    "decks = []\n",
    "for i in np.arange(np.count_nonzero(dir_names)):\n",
    "    with open(str(curdir) + '/' + str(dir_names[i])) as file:\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "    cards = soup.find(lambda tag: tag.name=='input' and tag.has_attr('name') and tag['name']=='c')\n",
    "    player = soup.find()\n",
    "    decks.append(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decks_dict = {\"deck_list\": decks}\n",
    "decks_clean = pd.DataFrame(decks_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
