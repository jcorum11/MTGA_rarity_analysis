{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all packages necessary for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import timeit\n",
    "import filecmp\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a webcrawler to gather the data from mtgtop8.com on competitive decks\n",
    "url = 'https://www.mtgtop8.com/format?f=ST&meta=52'\n",
    "url_standard_page_response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the response\n",
    "with open(os.path.abspath(os.curdir) + '/url_standard_page_response.html', mode='wb') as file:\n",
    "    file.write(url_standard_page_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open response as a BeautifulSoup object\n",
    "with open(os.path.abspath(os.curdir) + '/url_standard_page_response.html') as file:\n",
    "    soup = BeautifulSoup(file, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all urls for deck types and put them into a dataframe\n",
    "url = {'url': soup.find_all(href=re.compile(r\"archetype\\?a\"))}\n",
    "deck_type_urls = pd.DataFrame(data=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deck_type_urls.url a string and extract types from it and name the column 'type'\n",
    "deck_type_urls.url = deck_type_urls.url.astype('str')\n",
    "deck_type_urls['type'] = deck_type_urls.url.str.extract('(>.+<)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up 'type' column\n",
    "deck_type_urls.type = deck_type_urls.type.str.replace('>', '').str.replace('<', '').str.replace(' ', '_').str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract url from url column and add base url to it and clean it up a bit\n",
    "deck_type_urls.url = deck_type_urls.url.str.extract('(archetype\\?a.+f\\=ST)')\n",
    "deck_type_urls.url = 'https://www.mtgtop8.com/' + deck_type_urls.url\n",
    "deck_type_urls.url = deck_type_urls.url.str.replace('amp;', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create web crawler to request html pages from deck_type_urls\n",
    "# create new path and name it newpath\n",
    "newpath = os.path.abspath(os.curdir) + '/type_html_files'\n",
    "\n",
    "# check to make sure that there is no path that matches newpath and create a folder called type_html_files if there isn't\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "# change current directory to newpath\n",
    "os.chdir(newpath)\n",
    "\n",
    "# scrape all html files from urls in deck_type_urls and put them in type_html_files folder\n",
    "for i in np.arange(deck_type_urls.shape[0]):\n",
    "    type_html = requests.get(deck_type_urls.url[i])\n",
    "    with open(deck_type_urls.type[i] + '.html', 'wb') as file:\n",
    "        file.write(type_html.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of file names in type_html_files folder and name it dir_names\n",
    "dir_names = []\n",
    "cur_dir = os.path.abspath(os.curdir) + '/type_html_files'\n",
    "\n",
    "with os.scandir(cur_dir) as folder:\n",
    "    for file in folder: \n",
    "        if file.is_file():\n",
    "            dir_names.append(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all urls in all files\n",
    "players = []\n",
    "urls = []\n",
    "\n",
    "for i in np.arange(np.count_nonzero(dir_names)):\n",
    "    with open( os.path.abspath(os.curdir) + '/type_html_files/' + dir_names[i], encoding='latin_1') as file:\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "    for a in soup.find_all(href=re.compile(r'search\\?player')):\n",
    "        players.append(a)\n",
    "    for a in soup.find_all(href=re.compile(r'event\\?e\\=.+\\&d\\=.+\\&f\\=ST')):\n",
    "        urls.append(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put players and urls into a dataframe\n",
    "deck_urls = pd.DataFrame({'player': players, \n",
    "                         'url': urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up player and url columns\n",
    "deck_urls.player = deck_urls.player.astype('str')\n",
    "deck_urls.player = deck_urls.player.str.extract('(>.+<)')\n",
    "deck_urls.player = deck_urls.player.str.replace('>', '').str.replace('<', '').str.replace(' ', '_').str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract names from urls and add them to name column and clean it up\n",
    "deck_urls.url = deck_urls.url.astype('str')\n",
    "names = deck_urls.url.str.extract('(>.+<)')\n",
    "deck_urls['name'] = names\n",
    "deck_urls.name = deck_urls.name.str.replace('>', '').str.replace('<', '').str.replace(' ', '_').str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up url column\n",
    "deck_urls.url = deck_urls.url.str.extract('(event.+f\\=ST)')\n",
    "deck_urls.url = deck_urls.url.str.replace('amp;', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add url root to url column\n",
    "deck_urls.url = 'https://www.mtgtop8.com/' + deck_urls.url\n",
    "\n",
    "# back up one folder\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a web crawler to request html pages from deck_type_urls\n",
    "\n",
    "# create a new path and name it newpath\n",
    "newpath = os.path.abspath(os.curdir) + '/html_files'\n",
    "\n",
    "# check to make sure that there is no path that matches newpath and create a folder called html_files if there isn't\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "# change the current directory to newpath\n",
    "os.chdir(newpath)\n",
    "\n",
    "# scrape all html files from urls in deck_urls and put them in html_files folder\n",
    "deck_errors = []\n",
    "\n",
    "for i in np.arange(deck_urls.shape[0]):\n",
    "    try:\n",
    "        deck_html = requests.get(deck_urls.url[i])\n",
    "        with open(str(deck_urls.name[i]) + '_' + str(deck_urls.player[i]) + '.html', 'wb') as file:\n",
    "            file.write(deck_html.content)\n",
    "    except Exception as e:\n",
    "        print(str(i) + ' ' + str(e))\n",
    "        deck_errors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all directory names and put them in a list\n",
    "dir_names = []\n",
    "curdir = os.path.abspath(os.curdir) + '/html_files'\n",
    "\n",
    "with os.scandir(curdir) as folder:\n",
    "    for file in folder:\n",
    "        if file.is_file():\n",
    "            dir_names.append(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take names and add them to curdir and open them and find cards and players with beautifulsoup \n",
    "deck_lists = []\n",
    "players = []\n",
    "\n",
    "for i in np.arange(np.count_nonzero(dir_names)):\n",
    "    with open(str(curdir) + '/' + str(dir_names[i]), encoding='latin_1') as file:\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "    deck_list = soup.find(lambda tag: tag.name=='input' and tag.has_attr('name') and tag['name']=='c')\n",
    "    player = soup.find(lambda tag: tag.name=='a' and tag.has_attr('class'))\n",
    "    deck_lists.append(deck_list)\n",
    "    players.append(player)\n",
    "\n",
    "decks = pd.DataFrame({'deck_list': deck_lists, \n",
    "                      'player': players})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up decks 'player' column\n",
    "decks.player = decks.player.astype('str')\n",
    "decks.player = decks.player.str.extract('(>.+<)')\n",
    "decks.player = decks.player.str.replace('>', '').str.replace('<', '').str.replace(' ', '_').str.replace('/', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up deck_list column\n",
    "decks.deck_list = decks.deck_list.astype('str')\n",
    "decks.deck_list = decks.deck_list.str.replace('<input name=\"c\" type=\"hidden\" value=\"', '').str.replace('||\"/>', '').str.replace('\\|\\|', ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join decks with deck_urls\n",
    "decks = decks.merge(deck_urls, how='outer', on='player')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "decks.to_csv('decks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/steadyjingo/Documents/MTGA_rarity_analysis/url_standard_page_response.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1b1b2bed07b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/url_standard_page_response.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/type_html_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/html_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/steadyjingo/Documents/MTGA_rarity_analysis/url_standard_page_response.html'"
     ]
    }
   ],
   "source": [
    "os.remove(os.path.abspath(os.curdir) + '/url_standard_page_response.html')\n",
    "shutil.rmtree(os.path.abspath(os.curdir) + '/type_html_files')\n",
    "shutil.rmtree(os.path.abspath(os.curdir) + '/html_files')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
